{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización y escalabilidad en NumPy\n",
    "\n",
    "** Hoy en día, los conjuntos de datos crecen a un ritmo exponencial, y las empresas necesitan procesar grandes volúmenes de datos de manera eficiente. Como analista o científico de datos, es esencial aprender a manejar y optimizar el uso de arrays grandes para garantizar un buen rendimiento. **\n",
    "\n",
    "- En esta clase, aprenderás estrategias clave para manejar arrays grandes, optimizar el uso de memoria y realizar operaciones paralelas utilizando múltiples núcleos.\n",
    "\n",
    "### Dividir y Conquistar\n",
    "- Una estrategia eficaz para manejar grandes volúmenes de datos es dividir los arrays en bloques más pequeños. Esto reduce la carga de memoria y facilita el procesamiento.\n",
    "\n",
    "- Imagina que estás trabajando con el histórico de ventas de una tienda. Procesar toda la información de una vez puede ser impráctico, pero dividir los datos en bloques más pequeños puede hacer una gran diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crear un array grande de un millón de elementos aleatorios\n",
      "[0.92439182 0.01013722 0.84671365 ... 0.19254298 0.13228076 0.91548535]\n",
      "Dividir en bloques más pequeños\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 1\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 2\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 3\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 4\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 5\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 6\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 7\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 8\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 9\n",
      "Realizar operaciones en el bloque\n",
      "Procesando bloque 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Crear un array grande de un millón de elementos aleatorios')\n",
    "large_array = np.random.rand(1000000)\n",
    "print(large_array)\n",
    "print('Dividir en bloques más pequeños')\n",
    "block_size = 100000\n",
    "for i in range(0, len(large_array), block_size):\n",
    "    block = large_array[i:i+block_size]\n",
    "    print('Realizar operaciones en el bloque')\n",
    "    print(f\"Procesando bloque {i//block_size + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Este código divide un array grande en bloques de 100,000 elementos para procesarlos por partes, evitando agotar la memoria del sistema. Esta técnica es fundamental para manejar grandes volúmenes de datos de manera eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de Memoria y Optimización\n",
    "- Optimizar el uso de memoria es crucial cuando se trabaja con grandes volúmenes de datos.\n",
    "\n",
    "### Aquí te comparto dos estrategias efectivas:\n",
    "\n",
    "### Tipos de Datos Eficientes\n",
    "< Utilizar tipos de datos más eficientes puede reducir significativamente el uso de memoria. Por ejemplo, cambiar de float64 a float32 puede reducir el tamaño del array a la mitad, lo que es crucial cuando se trabaja con grandes volúmenes de datos >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array con tipo de dato float64\n",
      "Uso de memoria float64: 24\n",
      "Array con tipo de dato float32\n",
      "Uso de memoria float32: 12\n"
     ]
    }
   ],
   "source": [
    "print('Array con tipo de dato float64')\n",
    "array_float64 = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "print(\"Uso de memoria float64:\", array_float64.nbytes)\n",
    "\n",
    "print('Array con tipo de dato float32')\n",
    "array_float32 = np.array([1.0, 2.0, 3.0], dtype=np.float32)\n",
    "print(\"Uso de memoria float32:\", array_float32.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En este ejemplo, se crean dos arrays: uno de tipo float64 y otro de tipo float32. Usar float32 en lugar de float64 puede reducir el uso de memoria a la mitad, lo que es esencial para eficientar el manejo de grandes datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones In-place\n",
    "- Realizar operaciones in-place puede reducir el uso de memoria al evitar la creación de arrays temporales. Esto es especialmente útil cuando se necesita actualizar los valores de un array sin duplicarlo en la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array después de operación in-place: [2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "array = np.array([1, 2, 3, 4, 5])\n",
    "array += 1  # Operación in-place\n",
    "print(\"Array después de operación in-place:\", array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con +=1 realizamos una operación in-place sobre un array, incrementando cada elemento en 1. Las operaciones in-place modifican el array original sin crear uno nuevo, lo que ahorra memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones Paralelas y Uso de Múltiples Núcleos\n",
    "- Además de optimizar el uso de memoria, es fundamental acelerar el procesamiento de grandes arrays. Para ello, es esencial utilizar operaciones paralelas y múltiples núcleos.\n",
    "\n",
    "## Uso de numexpr para Operaciones Paralelas\n",
    "- numexpr es una biblioteca que permite realizar operaciones numéricas de manera más eficiente utilizando múltiples núcleos. Esto puede acelerar significativamente el procesamiento de grandes arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "Operación paralela completada con numexpr\n",
      "[1.42286476 0.88215053 0.62658693 ... 0.6882158  1.00449549 1.29191589]\n"
     ]
    }
   ],
   "source": [
    "import numexpr\n",
    "print(numexpr.__version__) \n",
    "import numpy as np\n",
    "\n",
    "# Crear dos arrays grandes\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "# Usando numexpr para operación paralela\n",
    "result = ne.evaluate('a + b')\n",
    "\n",
    "print(\"Operación paralela completada con numexpr\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Este código compara el tiempo de ejecución de una operación de suma en arrays grandes utilizando numexpr para realizar la operación en paralelo y sin numexpr.\n",
    "\n",
    "- Usar numexpr puede acelerar significativamente el procesamiento, lo que es crucial para manejar grandes volúmenes de datos de manera eficiente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de joblib para Paralelización\n",
    "- joblib f facilita la paralelización de tareas, permitiendo distribuir el trabajo entre múltiples núcleos del procesador. Esto es útil para tareas que pueden dividirse en partes independientes, como el procesamiento de grandes arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de la paralelización: [np.float64(50059.43547996441), np.float64(50017.91900940519), np.float64(50078.95277319463), np.float64(49880.44696716075), np.float64(50036.402467660475), np.float64(49983.59815786938), np.float64(49930.330993503514), np.float64(49797.08371100296), np.float64(49887.85173457208), np.float64(49943.289461673216)]\n",
      "\n",
      "Resultados block_size 100000\n",
      "\n",
      "Resultados Blocks [array([0.6973004 , 0.00975337, 0.62331281, ..., 0.79817132, 0.74830958,\n",
      "       0.71111353]), array([0.91588597, 0.27991975, 0.65574862, ..., 0.19826763, 0.79367015,\n",
      "       0.02048214]), array([0.73166512, 0.08719355, 0.41713654, ..., 0.1214011 , 0.81654867,\n",
      "       0.33649461]), array([0.84532869, 0.56297695, 0.26881863, ..., 0.2517778 , 0.69239014,\n",
      "       0.84343008]), array([0.74247566, 0.02710402, 0.1319845 , ..., 0.0722165 , 0.1381691 ,\n",
      "       0.82031859]), array([0.14399665, 0.86114645, 0.79068814, ..., 0.1400996 , 0.89122641,\n",
      "       0.9410229 ]), array([0.31481184, 0.07713921, 0.47016697, ..., 0.87305781, 0.15917763,\n",
      "       0.22913342]), array([0.15307834, 0.14842072, 0.25242262, ..., 0.44047943, 0.51580503,\n",
      "       0.68254557]), array([0.19398264, 0.28303893, 0.50278984, ..., 0.21322087, 0.19004578,\n",
      "       0.22991085]), array([0.37870879, 0.49860793, 0.01197342, ..., 0.58417723, 0.32934427,\n",
      "       0.38116397])]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_block(block):\n",
    "    return np.sum(block)\n",
    "\n",
    "large_array = np.random.rand(1000000)\n",
    "block_size = 100000\n",
    "blocks = [large_array[i:i+block_size] for i in range(0, len(large_array), block_size)]\n",
    "results = Parallel(n_jobs=-1)(delayed(process_block)(block) for block in blocks)\n",
    "print(\"\\nResultados de la paralelización:\", results)\n",
    "print('\\nResultados block_size' ,block_size)\n",
    "print('\\nResultados Blocks', blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizamos joblibpara paralelizar el procesamiento de un array grande dividido en bloques. Cada bloque se suma de forma paralela en múltiples núcleos del procesador, mejorando la eficiencia del procesamiento.\n",
    "\n",
    "- Manejar y optimizar arrays grandes es crucial en el análisis de datos para garantizar un rendimiento eficiente. Estrategias como dividir arrays en bloques, utilizar tipos de datos eficientes y realizar operaciones paralelas pueden mejorar significativamente la velocidad y eficiencia del procesamiento de datos. Estas técnicas son esenciales en diversos campos, desde la bioinformática hasta el análisis financiero, permitiendo el manejo eficaz de grandes volúmenes de datos.\n",
    "\n",
    "- Estas herramientas y estrategias te permitirán manejar datos de manera más eficiente, lo cual es fundamental en el mundo de la ciencia de datos. Practicar estos conceptos y técnicas puede convertirte en un experto en la optimización y escalabilidad con NumPy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NpyPandaAnalisisFYNEKA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
